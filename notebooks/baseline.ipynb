{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNJPiDRXC/AuuuusoYia+7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitanyaafle/hedgehog/blob/main/notebooks/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HYJfHq6lB9Gd",
        "outputId": "d80a7acb-938b-403a-d4bc-c6fc68387d25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNOTEBOOK OVERVIEW:\\n1. Setup (mount Drive, install packages, check GPU)\\n2. Download model and data\\n3. Implement baseline safety classifier\\n4. Evaluate on SafetyBench\\n5. Save results to Drive\\n\\nRuntime: ~30-40 minutes on T4, ~15-20 minutes on A100\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Project Hedgehog: Uncertainty-Aware LLM Safety Monitor\n",
        "# Baseline LLM Safety Classifier\n",
        "# Goal: Establish baseline performance before adding uncertainty quantification\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "NOTEBOOK OVERVIEW:\n",
        "1. Setup (mount Drive, install packages, check GPU)\n",
        "2. Download model and data\n",
        "3. Implement baseline safety classifier\n",
        "4. Evaluate on SafetyBench\n",
        "5. Save results to Drive\n",
        "\n",
        "Runtime: ~30-40 minutes on T4, ~15-20 minutes on A100\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Environment Setup\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for persistence\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory\n",
        "PROJECT_DIR = '/content/drive/MyDrive/llm_safety_project'\n",
        "!mkdir -p {PROJECT_DIR}\n",
        "!mkdir -p {PROJECT_DIR}/data\n",
        "!mkdir -p {PROJECT_DIR}/results\n",
        "!mkdir -p {PROJECT_DIR}/models\n",
        "\n",
        "print(f\"âœ“ Project directory: {PROJECT_DIR}\")\n",
        "\n",
        "# Check GPU\n",
        "print(f\"\\n=== GPU Info ===\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU detected! Go to Runtime > Change runtime type > T4 GPU\")\n",
        "\n",
        "# Check Python version\n",
        "print(f\"\\nPython version: {sys.version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eALadD4mCjh6",
        "outputId": "0c41bb29-df6a-4872-ca1e-a84d82021f73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Project directory: /content/drive/MyDrive/llm_safety_project\n",
            "\n",
            "=== GPU Info ===\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "GPU Memory: 85.17 GB\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Install Dependencies (Clean - No Quantization Needed)\n",
        "# ============================================================================\n",
        "\n",
        "# With A100 40GB, we don't need bitsandbytes for 8B models\n",
        "# This avoids all the CUDA/compatibility issues\n",
        "\n",
        "!pip install -q --upgrade beartype>=0.16.2\n",
        "!pip install -q transformer-lens==1.17.0\n",
        "!pip install -q mapie==0.8.3\n",
        "!pip install -q accelerate==0.25.0\n",
        "\n",
        "# Skip sentence-transformers for now (we'll add it on Day 3 when needed)\n",
        "# This avoids unnecessary dependencies\n",
        "\n",
        "print(\"âœ“ Packages installed (optimized for A100)\")\n",
        "\n",
        "# Verify imports\n",
        "import mapie\n",
        "import transformer_lens\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "import torch\n",
        "\n",
        "print(\"âœ“ All imports successful\")\n",
        "print(f\"\\nðŸ“¦ Package versions:\")\n",
        "print(f\"  transformers: {__import__('transformers').__version__}\")\n",
        "# print(f\"  transformer-lens: {transformer_lens.__version__}\") # Removed as transformer_lens does not have __version__\n",
        "print(f\"  mapie: {mapie.__version__}\")\n",
        "print(f\"  torch: {torch.__version__}\")\n",
        "\n",
        "print(f\"\\nðŸŽ® GPU Info:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"\\n  âœ… With {torch.cuda.get_device_properties(0).total_memory / 1e9:.0f}GB, you don't need quantization!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4RgzmPxCznI",
        "outputId": "1df7af75-ae80-426d-95fc-0c78e9b15683"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformer-lens 1.17.0 requires beartype<0.15.0,>=0.14.1, but you have beartype 0.22.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plum-dispatch 2.6.0 requires beartype>=0.16.2, but you have beartype 0.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ“ Packages installed (optimized for A100)\n",
            "âœ“ All imports successful\n",
            "\n",
            "ðŸ“¦ Package versions:\n",
            "  transformers: 4.57.3\n",
            "  mapie: 0.8.3\n",
            "  torch: 2.9.0+cu126\n",
            "\n",
            "ðŸŽ® GPU Info:\n",
            "  CUDA available: True\n",
            "  CUDA version: 12.6\n",
            "  GPU: NVIDIA A100-SXM4-80GB\n",
            "  Memory: 85.17 GB\n",
            "\n",
            "  âœ… With 85GB, you don't need quantization!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens"
      ],
      "metadata": {
        "id": "Ibu62oc6Ffwn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_lens."
      ],
      "metadata": {
        "id": "E-6BskcQFghP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}